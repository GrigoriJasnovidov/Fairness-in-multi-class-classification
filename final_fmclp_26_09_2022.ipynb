{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b6f0336a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Fitting is finished\n",
      "Predicting in process\n",
      "Predicting is finished\n"
     ]
    }
   ],
   "source": [
    "## general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from scipy.optimize import linprog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#this functions creates a synthetic dataset \n",
    "def synthetic_dataset(size = 1000, influence = True):\n",
    "    '''\n",
    "    size - number of observations \n",
    "    influence - if True, then the dependence between the sensitive attribute and label is imposed. If False is chosen, \n",
    "                than the sensitieve attribute is independent of the label.  \n",
    "    '''\n",
    "\n",
    "    def simple_splitter(arr):\n",
    "        arr_unchanged = arr.copy()\n",
    "        arr = np.sort(np.array(arr))\n",
    "        l = len(arr)\n",
    "        n1 = arr[int(l/3)]\n",
    "        n2 = arr[int(2*l/3)]\n",
    "        result = []\n",
    "        for i in range(l):\n",
    "            if arr_unchanged[i] <= n1:\n",
    "                result.append(0)\n",
    "            elif (arr_unchanged[i]>n1 and arr_unchanged[i] <= n2):\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(2)\n",
    "        result = np.array(result)\n",
    "        return result\n",
    "\n",
    "    attr = np.random.choice([0,1],size = size)\n",
    "    error_x = np.random.normal(loc=0.0, scale=0.3, size=size)\n",
    "    error_y = np.random.normal(loc=0.0, scale=0.3, size=size)\n",
    "    error_z = np.random.normal(loc=0.0, scale=0.3, size=size)\n",
    "    error_target = np.random.normal(loc=0.0, scale=0.5, size=size)\n",
    "\n",
    "    y1 = np.random.normal(loc= 1, scale = 1, size = size)\n",
    "    y2 = np.random.normal(loc= 1, scale = 1, size = size)\n",
    "    y3 = np.random.normal(loc= 1, scale = 1, size = size)\n",
    "\n",
    "    x = y1+y2+error_x\n",
    "    y = y1+y3+error_y\n",
    "    z = y2+y3+error_z\n",
    "\n",
    "    if influence == True:\n",
    "        target = x*(1+2*attr)+y*(1-0.5*attr)+z*(1+0.5*attr)+error_target*attr\n",
    "    if influence == False:\n",
    "        target = x+y+z+error_target\n",
    "    \n",
    "    target = simple_splitter(target)\n",
    "\n",
    "    synthetic_df = pd.DataFrame(np.array((x,y,z,attr,target))).T.rename(columns = {0:'x',1:'y',2:'z',3:'attr',4:'target'})  \n",
    "    \n",
    "    return synthetic_df\n",
    "\n",
    "\n",
    "# this function creates conditional use accuracy equality metric\n",
    "def cuae(y_true,y_pred, sensitive_features):    \n",
    "    '''\n",
    "    y_true - stands for the true label\n",
    "    y_pred - a forecast\n",
    "    sensitive_features - sensitive attribute\n",
    "    '''\n",
    "    true = np.array(y_true)\n",
    "    pred = np.array(y_pred)\n",
    "    protected = np.array(sensitive_features)\n",
    "    df = pd.DataFrame({'true':true, 'pred':pred, 'protected':protected}).astype('category')\n",
    "    classes = df['true'].drop_duplicates()\n",
    "    protected_groups_values = df['protected'].drop_duplicates()  \n",
    "    np_ans = np.zeros(shape = [len(protected_groups_values),len(classes)])\n",
    "    for j in range(len(protected_groups_values)):\n",
    "        for i in range(len(classes)):\n",
    "            protected_value = protected_groups_values[protected_groups_values.index[j]]\n",
    "            current_part = df[df['protected']==protected_value]\n",
    "            ndf = current_part[(current_part['true'] == classes[classes.index[i]])]\n",
    "            res = accuracy_score(ndf['true'],ndf['pred'])\n",
    "            np_ans[j,i] = res\n",
    "    df = pd.DataFrame(np_ans, columns = np.array(classes), index = np.array(protected_groups_values))\n",
    "    \n",
    "    max_diff = []\n",
    "    max_ratio = []\n",
    "\n",
    "    for i in df.columns:\n",
    "        column = df[i]\n",
    "        sort = np.array(column.sort_values())\n",
    "        max_ratio.append(sort[-1]/sort[0])\n",
    "        max_diff.append(sort[-1]-sort[0])\n",
    "    max_diff = np.array(max_diff)\n",
    "    max_ratio = np.array(max_ratio)\n",
    "    total_diff = max_diff.max()\n",
    "    total_ratio = max_ratio.max()\n",
    "    global_max = df.max().max()\n",
    "    global_min = df.min().min()\n",
    "    variation = global_max - global_min\n",
    "    \n",
    "    ans = {'df':df,\n",
    "           'diff': total_diff,\n",
    "           'ratio': total_ratio,\n",
    "            'varitaion': variation}\n",
    "    return ans\n",
    "\n",
    "\n",
    "\n",
    "# This is the main function. It solves the relabelling programm and gives the result in terms of equalized odds metric.\n",
    "def fmclp(dataset, estimator, number_iterations = None, prefit = False, interior_classifier = 'rf',\n",
    "        verbose = False, multiplier=1, random_state = None):\n",
    "    \n",
    "    #Below are a few axiulary functions. The first function converts arrays \n",
    "    #of [0,0,1,  0,1,0,   0,0,1, ..... ] types to usual classes [0,1,2,....].\n",
    "    \n",
    "    def zeros_ones_to_classes(x,length = 3):\n",
    "        n = int(len(x)/length)\n",
    "        l = []\n",
    "        for i in range(n):\n",
    "            z = x[i*length:i*length+length]\n",
    "            l.append(z.argmax())\n",
    "        return np.array(l, dtype=int)\n",
    "\n",
    "    #Next function creates an answer. It gets array of predictions for the part where protected attribute is zero, \n",
    "    #an array of predictions where protected attribute is one and then a series that indicates the order of this \n",
    "    #observations. Than it merges these arrays to get an answer.\n",
    "    \n",
    "    def answer_creator(x,y,grouper):\n",
    "        x = np.array(x)         #array of 1\n",
    "        y = np.array(y)         #array of 0\n",
    "        grouper = np.array(grouper)\n",
    "        ans = []\n",
    "        x_ind = 0\n",
    "        y_ind = 0\n",
    "        l = len(grouper)\n",
    "        for i in range(l):\n",
    "            if grouper[i] == 0:\n",
    "                ans.append(y[y_ind])\n",
    "                y_ind+=1\n",
    "            else:\n",
    "                ans.append(x[x_ind])\n",
    "                x_ind +=1\n",
    "        return np.array(ans)\n",
    "    \n",
    "    #The function below collects all neccessary information about the initial ml model\n",
    "    def ml_model(df,random_state = None, estimator = LGBMClassifier(), prefit = False):\n",
    "\n",
    "        y = df.drop('target',axis=1)\n",
    "        x = df['target']\n",
    "    \n",
    "        y_train,y_test,x_train,x_test = train_test_split(y,x, random_state = random_state)\n",
    "        if prefit ==False:\n",
    "            estimator.fit(y_train,x_train)\n",
    "        estimator_pred= estimator.predict(y_test)\n",
    "        accuracy_estimator = accuracy_score(estimator_pred,x_test)\n",
    " \n",
    "        zero_train_features = y_train[y_train['attr']==0]\n",
    "        one_train_features = y_train[y_train['attr']==1]\n",
    "        zero_train_labels = x_train[zero_train_features.index]\n",
    "        one_train_labels = x_train[one_train_features.index]\n",
    "    \n",
    "        zero_test_features = y_test[y_test['attr']==0]\n",
    "        one_test_features = y_test[y_test['attr']==1]\n",
    "    \n",
    "        zero_total = zero_train_features.shape[0]\n",
    "        one_total = one_train_features.shape[0]\n",
    "    \n",
    "        one_ratio = one_total/(one_total+zero_total)\n",
    "        zero_ratio = zero_total/(one_total+zero_total)\n",
    "        group = int(np.sqrt(one_total+zero_total))\n",
    "        one_group = int(one_ratio*group)\n",
    "        zero_group = int(zero_ratio*group)\n",
    "    \n",
    "        one_train_probs = pd.DataFrame(estimator.predict_proba(one_train_features)).rename(\n",
    "            columns = {0:'zero_class',1:'first_class', 2:'second_class'})\n",
    "        one_train_probs['label'] = np.array(one_train_labels)\n",
    "    \n",
    "        zero_train_probs = pd.DataFrame(estimator.predict_proba(zero_train_features)).rename(\n",
    "            columns = {0:'zero_class',1:'first_class', 2:'second_class'})\n",
    "        zero_train_probs['label'] = np.array(zero_train_labels)\n",
    "   \n",
    "        one_test_probs = pd.DataFrame(estimator.predict_proba(one_test_features)).rename(\n",
    "            columns = {0:'zero_class',1:'first_class', 2:'second_class'})\n",
    "    \n",
    "        zero_test_probs = pd.DataFrame(estimator.predict_proba(zero_test_features)).rename(\n",
    "            columns = {0:'zero_class',1:'first_class', 2:'second_class'})\n",
    "\n",
    "        d_ans = {'dataset': df,\n",
    "                 'estimator': estimator,\n",
    "                 'y_train':y_train,\n",
    "                 'y_test': y_test,\n",
    "                 'x_train': x_train,\n",
    "                 'x_test': x_test,\n",
    "                 'predictions':  estimator_pred,\n",
    "                 'estimator_accuracy': accuracy_estimator,\n",
    "                 'group': group,\n",
    "                 'one_group': one_group,\n",
    "                 'zero_group': zero_group,\n",
    "                 'one_train_probs': one_train_probs,\n",
    "                 'zero_train_probs': zero_train_probs,\n",
    "                 'one_test_probs': one_test_probs,\n",
    "                 'zero_test_probs': zero_test_probs\n",
    "                 }\n",
    "        return d_ans\n",
    "    \n",
    "    \n",
    "    #The function below is the core of our approach. It solves the linear programm and force the classifier to be fair.\n",
    "    \n",
    "    def lp_solver(d, number_iterations = 10, classifier = RandomForestClassifier(), verbose = False, multiplier = 1,\n",
    "             random_state = None):\n",
    "\n",
    "        group = multiplier*d['group']\n",
    "        one_group = multiplier*d['one_group']\n",
    "        zero_group = multiplier*d['zero_group']\n",
    "\n",
    "        one_train_probs = d['one_train_probs']\n",
    "        zero_train_probs = d['zero_train_probs']   \n",
    "    \n",
    "        bounds = []\n",
    "        for i in range(3*one_group+3*zero_group):\n",
    "            bounds.append((0,1))\n",
    "\n",
    "        equation_vector = [1]*(one_group+zero_group)\n",
    "        for i in range(3):\n",
    "            equation_vector.append(0)\n",
    "\n",
    "        equation_matrix0 = np.zeros((one_group+zero_group,3*one_group+3*zero_group))\n",
    "        for i in range(one_group+zero_group):\n",
    "            equation_matrix0[i,3*i] = 1\n",
    "            equation_matrix0[i,3*i+1] = 1\n",
    "            equation_matrix0[i,3*i+2] = 1\n",
    "        equation_matrix0 = np.array(equation_matrix0)\n",
    "\n",
    "        equation_vector = [1]*(one_group+zero_group)\n",
    "        for i in range(3):\n",
    "            equation_vector.append(0)\n",
    "\n",
    "        one_predictor_array =[]\n",
    "        zero_predictor_array = []\n",
    "\n",
    "        if verbose:\n",
    "            print('Start fitting')\n",
    "        for k in range(number_iterations):\n",
    "            if random_state == None:\n",
    "                one_sample = d['one_train_probs'].sample(one_group)\n",
    "                zero_sample = d['zero_train_probs'].sample(zero_group)\n",
    "            else:\n",
    "                one_sample = d['one_train_probs'].sample(one_group, random_state = k)\n",
    "                zero_sample = d['zero_train_probs'].sample(zero_group, random_state = k)\n",
    "            #I0, I1, I2 labels:\n",
    "            I0 = one_sample[one_sample['label']==0]\n",
    "            I1 = one_sample[one_sample['label']==1]\n",
    "            I2 = one_sample[one_sample['label']==2]\n",
    "\n",
    "            #J0, J1, J2 labels:\n",
    "            J0 = zero_sample[zero_sample['label']==0]\n",
    "            J1 = zero_sample[zero_sample['label']==1]\n",
    "            J2 = zero_sample[zero_sample['label']==2]\n",
    "\n",
    "            lenI0 = len(I0)\n",
    "            lenI1 = len(I1)\n",
    "            lenI2 = len(I2)\n",
    "            lenJ0 = len(J0)\n",
    "            lenJ1 = len(J1)\n",
    "            lenJ2 = len(J2)\n",
    "\n",
    "            vectorI0 = [ ]\n",
    "            vectorI1 = [ ]\n",
    "            vectorI2 = [ ]\n",
    "            for i in one_sample.index:\n",
    "                if i in I0.index:\n",
    "                    vectorI0.append(lenJ0)\n",
    "                    vectorI0.append(0)\n",
    "                    vectorI0.append(0)\n",
    "                else:\n",
    "                    vectorI0.append(0)\n",
    "                    vectorI0.append(0)\n",
    "                    vectorI0.append(0)\n",
    "            for i in one_sample.index:\n",
    "                if i in I1.index:\n",
    "                    vectorI1.append(0)\n",
    "                    vectorI1.append(lenJ1)\n",
    "                    vectorI1.append(0)\n",
    "                else:\n",
    "                    vectorI1.append(0)\n",
    "                    vectorI1.append(0)\n",
    "                    vectorI1.append(0)\n",
    "            for i in one_sample.index:\n",
    "                if i in I2.index:\n",
    "                    vectorI2.append(0)\n",
    "                    vectorI2.append(0)\n",
    "                    vectorI2.append(lenJ2)\n",
    "                else:\n",
    "                    vectorI2.append(0)\n",
    "                    vectorI2.append(0)\n",
    "                    vectorI2.append(0)\n",
    "            vectorI0 = np.array(vectorI0)\n",
    "            vectorI1 = np.array(vectorI1)\n",
    "            vectorI2 = np.array(vectorI2)\n",
    "\n",
    "            vectorJ0 = [ ]\n",
    "            vectorJ1 = [ ]\n",
    "            vectorJ2 = [ ]\n",
    "\n",
    "            for i in zero_sample.index:\n",
    "                if i in J0.index:\n",
    "                    vectorJ0.append(-lenI0)\n",
    "                    vectorJ0.append(0)\n",
    "                    vectorJ0.append(0)\n",
    "                else:\n",
    "                    vectorJ0.append(0)\n",
    "                    vectorJ0.append(0)\n",
    "                    vectorJ0.append(0)\n",
    "            for i in zero_sample.index:\n",
    "                if i in J1.index:\n",
    "                    vectorJ1.append(0)\n",
    "                    vectorJ1.append(-lenI1)\n",
    "                    vectorJ1.append(0)\n",
    "                else:\n",
    "                    vectorJ1.append(0)\n",
    "                    vectorJ1.append(0)\n",
    "                    vectorJ1.append(0)\n",
    "            for i in zero_sample.index:\n",
    "                if i in J2.index:\n",
    "                    vectorJ2.append(0)\n",
    "                    vectorJ2.append(0)\n",
    "                    vectorJ2.append(-lenI2)\n",
    "                else:\n",
    "                    vectorJ2.append(0)\n",
    "                    vectorJ2.append(0)\n",
    "                    vectorJ2.append(0)\n",
    "            vectorJ0 = np.array(vectorJ0)\n",
    "            vectorJ1 = np.array(vectorJ1)\n",
    "            vectorJ2 = np.array(vectorJ2)\n",
    "\n",
    "            row0 =  np.concatenate((vectorI0, vectorJ0)).reshape(1,-1)\n",
    "            row1 =  np.concatenate((vectorI1, vectorJ1)).reshape(1,-1)\n",
    "            row2 =  np.concatenate((vectorI2, vectorJ2)).reshape(1,-1)\n",
    "            rows = np.concatenate((row0,row1,row2),axis=0)\n",
    "\n",
    "            equation_matrix = np.concatenate((equation_matrix0,rows),axis=0)\n",
    "\n",
    "            C = np.array(one_sample[['zero_class', 'first_class', 'second_class']]).ravel()\n",
    "            B = np.array(zero_sample[['zero_class', 'first_class', 'second_class']]).ravel()\n",
    "            objective = (-1)*np.concatenate((C,B))\n",
    "            array = linprog(\n",
    "                    c = objective, A_ub=None, b_ub=None, \n",
    "                       A_eq=equation_matrix, \n",
    "                       b_eq=equation_vector, \n",
    "                    bounds=bounds, method='highs-ipm', callback=None, options=None, x0=None).x\n",
    "\n",
    "            fair_pred = zeros_ones_to_classes(array)\n",
    "            fair_pred_one = fair_pred[:one_group]\n",
    "            fair_pred_zero = fair_pred[one_group:]\n",
    "\n",
    "            # here we prepare classes to relabeling\n",
    "            one_df = pd.DataFrame(one_sample, columns = ['zero_class', 'first_class','second_class'])     \n",
    "            one_predictor = classifier\n",
    "            one_predictor.fit(one_df, fair_pred_one)\n",
    "            one_predictor_array.append(one_predictor)\n",
    "        \n",
    "            zero_df = pd.DataFrame(zero_sample, columns = ['zero_class', 'first_class','second_class'])    \n",
    "            zero_predictor = classifier\n",
    "            zero_predictor.fit(zero_df,fair_pred_zero)\n",
    "            zero_predictor_array.append(zero_predictor)\n",
    "            if verbose:\n",
    "                print(k+1)\n",
    "        ans = {'one_predictor_array':one_predictor_array,\n",
    "               'zero_predictor_array': zero_predictor_array}\n",
    "        if verbose:\n",
    "            print('Fitting is finished')\n",
    "        return ans\n",
    "    \n",
    "    #The function below is used for forecasting.\n",
    "    \n",
    "    def predictor(solved,d,verbose=False):\n",
    "        if verbose:\n",
    "            print('Predicting in process')\n",
    "        one_predictor_array = solved['one_predictor_array']\n",
    "        zero_predictor_array = solved['zero_predictor_array']\n",
    "\n",
    "        one_probs = d['one_test_probs']\n",
    "        zero_probs = d['zero_test_probs']\n",
    "        one_rows = one_probs.shape[0]\n",
    "        zero_rows = zero_probs.shape[0]\n",
    "        one_cols = len(one_predictor_array)\n",
    "        zero_cols = len(zero_predictor_array)\n",
    "\n",
    "        one_final_array = np.empty(shape = (one_cols,one_rows))\n",
    "        for i in range(one_cols):\n",
    "            one_final_array[i] = one_predictor_array[i].predict(one_probs)\n",
    "        one_final_array = pd.DataFrame(one_final_array)\n",
    "\n",
    "        one_final_ans = []\n",
    "        for i in range(one_rows):\n",
    "            one_final_ans.append(one_final_array[i].value_counts().sort_values(ascending = False).index[0])\n",
    "        \n",
    "        zero_final_array = np.empty(shape = (zero_cols,zero_rows))\n",
    "        for i in range(zero_cols):\n",
    "            zero_final_array[i] = zero_predictor_array[i].predict(zero_probs)\n",
    "        zero_final_array = pd.DataFrame(zero_final_array)\n",
    "\n",
    "        zero_final_ans = []\n",
    "        for i in range(zero_rows):\n",
    "            zero_final_ans.append(zero_final_array[i].value_counts().sort_values(ascending = False).index[0])\n",
    "    \n",
    "        preds = answer_creator(one_final_ans, zero_final_ans, d['y_test']['attr'] )\n",
    "    \n",
    "        ans = {'one_preds': one_final_ans,\n",
    "               'zero_preds': zero_final_ans,\n",
    "               'preds': preds}\n",
    "        if verbose:\n",
    "            print('Predicting is finished')\n",
    "       \n",
    "        return ans\n",
    "    \n",
    "    \n",
    "    #Now are ready to implement our algorithm.\n",
    "    \n",
    "    interior_classifier_dict = {'rf':RandomForestClassifier(random_state  = random_state),\n",
    "                                'lr': LogisticRegression(),\n",
    "                                'dt': DecisionTreeClassifier(random_state = random_state),\n",
    "                                'svm':SVC(),\n",
    "                                'lgb':LGBMClassifier(),\n",
    "                                'knn':KNeighborsClassifier(n_neighbors=3)}\n",
    "    \n",
    "    model = ml_model(df = dataset, estimator = estimator, random_state = random_state, prefit = prefit)\n",
    "    solved = lp_solver(model,\n",
    "                       classifier =  interior_classifier_dict[interior_classifier],\n",
    "                       number_iterations=number_iterations,\n",
    "                       verbose = verbose, \n",
    "                       multiplier=multiplier,\n",
    "                       random_state = random_state)\n",
    "    pred = predictor(solved, model,verbose)\n",
    "    fair_cuae = cuae(y_true = model['x_test'], y_pred = pred['preds'], \n",
    "                                           sensitive_features = model['y_test']['attr'])\n",
    "    fair_accuracy = accuracy_score(pred['preds'],model['x_test'])\n",
    "    \n",
    "    ans = {'accuracy_of_initial_classifier': model['estimator_accuracy'],\n",
    "           'fairness_of_initial_classifier': cuae(y_true = model['x_test'], y_pred = model['predictions'],\n",
    "                                                                    sensitive_features = model['y_test']['attr']),\n",
    "           'solved' : solved,\n",
    "           'predictions': pred,\n",
    "           'fairness_of_fair_classifier': fair_cuae,\n",
    "           'accuracy_of_fair_classifier':fair_accuracy,\n",
    "           'dataset':dataset,\n",
    "           'model':model,\n",
    "           'multiplier':multiplier,\n",
    "           'interior_classifier':interior_classifier\n",
    "           }\n",
    "    \n",
    "    return ans\n",
    "\n",
    "# example \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    d = synthetic_dataset(400)\n",
    "    dataset = synthetic_dataset(20000)    \n",
    "    random_state = 78\n",
    "    cl = LGBMClassifier(random_state= random_state)\n",
    "    y = d.drop('target',axis=1)\n",
    "    x = d['target']\n",
    "    y_train,y_test,x_train,x_test = train_test_split(y,x, random_state = random_state)\n",
    "    cl.fit(y_train,x_train)\n",
    "    res = fmclp(dataset = dataset, \n",
    "           estimator = cl, \n",
    "           number_iterations = 10, \n",
    "           prefit = True, \n",
    "           interior_classifier = 'knn',\n",
    "           verbose = True, \n",
    "           multiplier=30, \n",
    "           random_state = random_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
